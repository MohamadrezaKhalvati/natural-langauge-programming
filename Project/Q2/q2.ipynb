{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "from nltk import FreqDist\n",
    "from nltk.util import bigrams\n",
    "from nltk import bigrams, FreqDist\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../Q1/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1157 entries, 0 to 1156\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   postUrl             1157 non-null   object\n",
      " 1   id                  1157 non-null   int64 \n",
      " 2   text                1156 non-null   object\n",
      " 3   ownerUsername       1157 non-null   object\n",
      " 4   ownerProfilePicUrl  1157 non-null   object\n",
      " 5   timestamp           1157 non-null   object\n",
      " 6   likesCount          1157 non-null   int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 63.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postUrl</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ownerUsername</th>\n",
       "      <th>ownerProfilePicUrl</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>likesCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>17981320055390052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n4i1er</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T01:46:41.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18225095332247021</td>\n",
       "      <td>üòçüòçüî•üî•üî•</td>\n",
       "      <td>farid.zand1997</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T09:30:36.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18016249762974314</td>\n",
       "      <td>patm</td>\n",
       "      <td>andreprivet_</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T12:32:20.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>17980123316614267</td>\n",
       "      <td>@hoccein_hemati68 ÿß€åŸÜ ⁄©Ÿàÿµ⁄©ÿ¥ ÿ™Ÿà ÿß€åÿ±ÿßŸÜ ÿ®ŸàÿØ ŸÜŸáÿß€åÿ™...</td>\n",
       "      <td>kurd___boy666</td>\n",
       "      <td>https://scontent-lga3-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T13:14:43.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18036756691566765</td>\n",
       "      <td>@amir_niarashid üí©ü§£üñï</td>\n",
       "      <td>kurd___boy666</td>\n",
       "      <td>https://scontent-lga3-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T13:15:08.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    postUrl                 id  \\\n",
       "0  https://www.instagram.com/p/Cz67N84Pezn/  17981320055390052   \n",
       "1  https://www.instagram.com/p/Cz67N84Pezn/  18225095332247021   \n",
       "2  https://www.instagram.com/p/Cz67N84Pezn/  18016249762974314   \n",
       "3  https://www.instagram.com/p/Cz67N84Pezn/  17980123316614267   \n",
       "4  https://www.instagram.com/p/Cz67N84Pezn/  18036756691566765   \n",
       "\n",
       "                                                text   ownerUsername  \\\n",
       "0                                                NaN          n4i1er   \n",
       "1                                              üòçüòçüî•üî•üî•  farid.zand1997   \n",
       "2                                               patm    andreprivet_   \n",
       "3  @hoccein_hemati68 ÿß€åŸÜ ⁄©Ÿàÿµ⁄©ÿ¥ ÿ™Ÿà ÿß€åÿ±ÿßŸÜ ÿ®ŸàÿØ ŸÜŸáÿß€åÿ™...   kurd___boy666   \n",
       "4                                @amir_niarashid üí©ü§£üñï   kurd___boy666   \n",
       "\n",
       "                                  ownerProfilePicUrl  \\\n",
       "0  https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "1  https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "2  https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "3  https://scontent-lga3-2.cdninstagram.com/v/t51...   \n",
       "4  https://scontent-lga3-2.cdninstagram.com/v/t51...   \n",
       "\n",
       "                  timestamp  likesCount  \n",
       "0  2023-12-09T01:46:41.000Z           0  \n",
       "1  2023-12-09T09:30:36.000Z           0  \n",
       "2  2023-12-09T12:32:20.000Z           0  \n",
       "3  2023-12-09T13:14:43.000Z           0  \n",
       "4  2023-12-09T13:15:08.000Z           0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove comments with empty text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove text that is not related to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\n",
    "    data[\"text\"].apply(lambda x: all([token.pos_ != \"FOREIGN\" for token in nlp(x)]))\n",
    "    == True\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert timestamp to a standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non-ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: \"\".join([ch for ch in x if ord(ch) < 128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].str.replace(\"[^\\w\\s]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "data[\"text\"] = data[\"text\"].apply(\n",
    "    lambda x: \" \".join([word for word in x.split() if word not in stop_words])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"text\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows where the length of text is not greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: len(str(x)) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postUrl</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ownerUsername</th>\n",
       "      <th>ownerProfilePicUrl</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>likesCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18016249762974314</td>\n",
       "      <td>patm</td>\n",
       "      <td>andreprivet_</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09 12:32:20+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>17980123316614267</td>\n",
       "      <td>@hoccein_hemati68 !</td>\n",
       "      <td>kurd___boy666</td>\n",
       "      <td>https://scontent-lga3-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09 13:14:43+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18036756691566765</td>\n",
       "      <td>@amir_niarashid</td>\n",
       "      <td>kurd___boy666</td>\n",
       "      <td>https://scontent-lga3-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09 13:15:08+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18118091365334462</td>\n",
       "      <td>nice arms</td>\n",
       "      <td>st3ph_3s0n</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09 20:37:29+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18247697080171707</td>\n",
       "      <td>gostoso</td>\n",
       "      <td>pedr0_hgc</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-10 01:28:48+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>https://www.instagram.com/p/C17pLDogJUV/</td>\n",
       "      <td>18009715463508895</td>\n",
       "      <td>@cbum transformation lost 25 kg year @klemensh...</td>\n",
       "      <td>klemensheidorn</td>\n",
       "      <td>https://instagram.ferz2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>2024-01-17 12:35:51+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>https://www.instagram.com/p/C17pLDogJUV/</td>\n",
       "      <td>17996526734407530</td>\n",
       "      <td>get favor</td>\n",
       "      <td>aftabkhan_editz</td>\n",
       "      <td>https://instagram.ferz2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>2024-01-17 12:37:10+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>https://www.instagram.com/p/C17pLDogJUV/</td>\n",
       "      <td>18032966065693847</td>\n",
       "      <td>eu consigo coar costas</td>\n",
       "      <td>jpzff_11</td>\n",
       "      <td>https://instagram.ferz2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>2024-01-17 13:05:14+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>https://www.instagram.com/p/C17pLDogJUV/</td>\n",
       "      <td>17911811417866846</td>\n",
       "      <td>@zeuswithcan sen daha iyisin</td>\n",
       "      <td>emr3c3lik</td>\n",
       "      <td>https://instagram.ferz2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>2024-01-17 13:26:09+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>https://www.instagram.com/p/C17pLDogJUV/</td>\n",
       "      <td>18019793056964847</td>\n",
       "      <td>@loganlma0 nothing literally chemical body pro...</td>\n",
       "      <td>alexandrews5125</td>\n",
       "      <td>https://instagram.ferz2-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>2024-01-17 13:28:04+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>940 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       postUrl                 id  \\\n",
       "2     https://www.instagram.com/p/Cz67N84Pezn/  18016249762974314   \n",
       "3     https://www.instagram.com/p/Cz67N84Pezn/  17980123316614267   \n",
       "4     https://www.instagram.com/p/Cz67N84Pezn/  18036756691566765   \n",
       "5     https://www.instagram.com/p/Cz67N84Pezn/  18118091365334462   \n",
       "7     https://www.instagram.com/p/Cz67N84Pezn/  18247697080171707   \n",
       "...                                        ...                ...   \n",
       "1151  https://www.instagram.com/p/C17pLDogJUV/  18009715463508895   \n",
       "1152  https://www.instagram.com/p/C17pLDogJUV/  17996526734407530   \n",
       "1153  https://www.instagram.com/p/C17pLDogJUV/  18032966065693847   \n",
       "1154  https://www.instagram.com/p/C17pLDogJUV/  17911811417866846   \n",
       "1155  https://www.instagram.com/p/C17pLDogJUV/  18019793056964847   \n",
       "\n",
       "                                                   text    ownerUsername  \\\n",
       "2                                                  patm     andreprivet_   \n",
       "3                                   @hoccein_hemati68 !    kurd___boy666   \n",
       "4                                       @amir_niarashid    kurd___boy666   \n",
       "5                                             nice arms       st3ph_3s0n   \n",
       "7                                               gostoso        pedr0_hgc   \n",
       "...                                                 ...              ...   \n",
       "1151  @cbum transformation lost 25 kg year @klemensh...   klemensheidorn   \n",
       "1152                                          get favor  aftabkhan_editz   \n",
       "1153                             eu consigo coar costas         jpzff_11   \n",
       "1154                       @zeuswithcan sen daha iyisin        emr3c3lik   \n",
       "1155  @loganlma0 nothing literally chemical body pro...  alexandrews5125   \n",
       "\n",
       "                                     ownerProfilePicUrl  \\\n",
       "2     https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "3     https://scontent-lga3-2.cdninstagram.com/v/t51...   \n",
       "4     https://scontent-lga3-2.cdninstagram.com/v/t51...   \n",
       "5     https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "7     https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "...                                                 ...   \n",
       "1151  https://instagram.ferz2-1.fna.fbcdn.net/v/t51....   \n",
       "1152  https://instagram.ferz2-1.fna.fbcdn.net/v/t51....   \n",
       "1153  https://instagram.ferz2-1.fna.fbcdn.net/v/t51....   \n",
       "1154  https://instagram.ferz2-1.fna.fbcdn.net/v/t51....   \n",
       "1155  https://instagram.ferz2-1.fna.fbcdn.net/v/t51....   \n",
       "\n",
       "                     timestamp  likesCount  \n",
       "2    2023-12-09 12:32:20+00:00           0  \n",
       "3    2023-12-09 13:14:43+00:00           0  \n",
       "4    2023-12-09 13:15:08+00:00           0  \n",
       "5    2023-12-09 20:37:29+00:00           0  \n",
       "7    2023-12-10 01:28:48+00:00           0  \n",
       "...                        ...         ...  \n",
       "1151 2024-01-17 12:35:51+00:00           1  \n",
       "1152 2024-01-17 12:37:10+00:00           0  \n",
       "1153 2024-01-17 13:05:14+00:00           0  \n",
       "1154 2024-01-17 13:26:09+00:00           0  \n",
       "1155 2024-01-17 13:28:04+00:00           1  \n",
       "\n",
       "[940 rows x 7 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocess_text(entry) for entry in data['text'] if pd.notna(entry) and isinstance(entry, str)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [item for sublist in corpus for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigram language model is a simple yet powerful statistical model used in natural language processing (NLP) to predict the probability of occurrence of a word in a sequence of words. It assumes that the probability of a word appearing in a sentence is independent of the context in which it appears. This assumption, while not entirely accurate, often provides a reasonable approximation of the true word distribution in a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class UnigramModel:\n",
    "    def __init__(self, smoothing=True):\n",
    "        self.word_counts = Counter()\n",
    "        self.total_words = 0\n",
    "        self.vocab_size = 0\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Count the frequency of each word in the corpus\n",
    "        self.word_counts = Counter(corpus)\n",
    "        self.total_words = len(corpus)\n",
    "        self.vocab_size = len(set(corpus))\n",
    "\n",
    "    def generate_sentence(self, length=10):\n",
    "        # Generate a random sentence based on the unigram model\n",
    "        sentence = [random.choice(list(self.word_counts.keys())) for _ in range(length)]\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def probability(self, word):\n",
    "        # Calculate the probability of a word based on the unigram model with smoothing\n",
    "        if self.total_words == 0:\n",
    "            return 0\n",
    "\n",
    "        if self.smoothing:\n",
    "            return (self.word_counts[word] + 1) / (self.total_words + self.vocab_size)\n",
    "        else:\n",
    "            return self.word_counts[word] / self.total_words\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        # Calculate perplexity of a sentence\n",
    "        words = sentence.split()\n",
    "        word_probabilities = [self.probability(word) for word in words]\n",
    "        perplexity = math.exp(-sum(math.log(prob, 2) for prob in word_probabilities) / len(words))\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnigramModel(smoothing=True)\n",
    "model.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate 5 sentences and calculate perplexity\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: bros viratkohli miles gnio brazo hacen support rara hairline mama.\n",
      "Perplexity: 98348.58\n",
      "Generated Sentence: stiff he involucran segurar dear mais prprio filter mi espalda.\n",
      "Perplexity: 64185.04\n",
      "Generated Sentence: tarining pictures attractive von lateral multiverse sensao ni hwa menos.\n",
      "Perplexity: 91361.27\n",
      "Generated Sentence: rodrigoamgoes ama segundo kkkk q personality hide feioooooo marido lagi.\n",
      "Perplexity: 69896.42\n",
      "Generated Sentence: whos it chega paga legenda sentiu tigre roids sos classic.\n",
      "Perplexity: 74800.28\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generated_sentence = model.generate_sentence()\n",
    "    perplexity = model.perplexity(generated_sentence)\n",
    "    print(f\"Generated Sentence: {generated_sentence}.\\nPerplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigram language model is a statistical language model that predicts the probability of a word appearing in a sequence of words based on the word that precedes it. Unlike unigram models, which assume that words occur independently of each other, bigram models take into account the sequential nature of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self, smoothing=True):\n",
    "        self.bigram_counts = Counter()\n",
    "        self.unigram_counts = Counter()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "\t # Tokenize and count unigrams and bigrams in the corpus\n",
    "    def train(self, corpus):  \n",
    "        tokenized_corpus = [word_tokenize(' '.join(sentence)) for sentence in corpus]\n",
    "        self.unigram_counts = Counter([word for sentence in tokenized_corpus for word in sentence])\n",
    "        self.bigram_counts = Counter(bigrams([word for sentence in tokenized_corpus for word in sentence]))\n",
    "\n",
    "\t# Generate a random sentence based on the bigram model\n",
    "    def generate_sentence(self, length=10, start_word=None):\n",
    "        sentence = []\n",
    "        current_word = start_word if start_word else random.choice(list(self.unigram_counts.keys()))\n",
    "        for _ in range(length):\n",
    "            sentence.append(current_word)\n",
    "            next_word_candidates = [word for word in self.unigram_counts.keys() if (current_word, word) in self.bigram_counts]\n",
    "            if next_word_candidates:\n",
    "                current_word = random.choice(next_word_candidates)\n",
    "            else:\n",
    "                break\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "\t# Calculate the probability of a word based on the bigram model with smoothing\n",
    "    def probability(self, word, previous_word=None):\n",
    "        if self.unigram_counts[previous_word] == 0:\n",
    "            return 0\n",
    "\n",
    "        if self.smoothing:\n",
    "            bigram_count = self.bigram_counts[(previous_word, word)]\n",
    "            unigram_count = self.unigram_counts[previous_word]\n",
    "            return (bigram_count + 1) / (unigram_count + len(self.unigram_counts))\n",
    "        else:\n",
    "            if (previous_word, word) in self.bigram_counts:\n",
    "                return self.bigram_counts[(previous_word, word)] / self.unigram_counts[previous_word]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "\t# Calculate perplexity of a sentence\n",
    "    def perplexity(self, sentence):\n",
    "        \n",
    "        words = word_tokenize(sentence)\n",
    "        word_probabilities = [self.probability(words[i], words[i - 1]) for i in range(1, len(words))]\n",
    "        word_probabilities = [prob if prob > 0 else 1e-10 for prob in word_probabilities]  # Avoid zero probabilities\n",
    "        perplexity = math.exp(-sum(math.log(prob, 2) for prob in word_probabilities) / (len(words) - 1)) if sum(word_probabilities) > 0 else float('inf')\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trigram language model is a statistical language model that predicts the probability of a word appearing in a sequence of words based on the two preceding words. It takes into account the sequential nature of language by considering the dependencies between three consecutive words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigramModel(token1, token2, token3, dataset, alpha=1):\n",
    "    token_count = Counter(dataset)\n",
    "    trigram_count = Counter(zip(dataset, dataset[1:], dataset[2:]))\n",
    "    bigram_count = Counter(zip(dataset, dataset[1:]))\n",
    "    return (trigram_count[(token1, token2, token3)] + alpha) / (bigram_count[(token1, token2)] + alpha * vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate perplexity base on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, n, dataset, alpha=1):\n",
    "    probabilities = [model(*dataset[i-n+1:i+1], dataset[:i], alpha) for i in range(n-1, len(dataset))]\n",
    "    perplexity = np.exp(-np.mean(np.log(probabilities)))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate sentence base on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, n, dataset, length=10):\n",
    "    sentence = []\n",
    "    for _ in range(length):\n",
    "        if n == 1:\n",
    "            next_token = random.choice(dataset)\n",
    "        else:\n",
    "            prev_tokens = sentence[-n+1:] if n > 1 else []\n",
    "            candidates = [token for token in set(dataset) if model(*(prev_tokens + [token]), dataset) > 0]\n",
    "            next_token = random.choice(candidates)\n",
    "        sentence.append(next_token)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate sentence perplexity base on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_perplexity(model, n, sentence, dataset, alpha=1):\n",
    "    tokens = tokenize_the_text(sentence)\n",
    "    probabilities = [model(*tokens[i-n+1:i+1], dataset, alpha) for i in range(n-1, len(tokens))]\n",
    "    perplexity = np.exp(-np.mean(np.log(probabilities)))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_report(model, n, dataset, alpha=1, num_sentences=5):\n",
    "    print(f\"Generating and evaluating {num_sentences} sentences for {n}-gram model:\")\n",
    "    for i in range(num_sentences):\n",
    "        generated_sentence = generate_sentence(model, n, dataset)\n",
    "        perplexity = calculate_sentence_perplexity(model, n, generated_sentence, dataset, alpha)\n",
    "        print(f\"Generated Sentence {i+1}: {generated_sentence}\")\n",
    "        print(f\"Perplexity: {perplexity}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram model sentence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pay attention : i got error in developing bigram and trigram model , so i have remove it's model's code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
