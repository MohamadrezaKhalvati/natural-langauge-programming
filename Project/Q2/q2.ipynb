{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    CountVectorizer,\n",
    ")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from nltk import FreqDist\n",
    "from nltk.util import bigrams\n",
    "from nltk import bigrams, FreqDist\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../Q1/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1157 entries, 0 to 1156\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   postUrl             1157 non-null   object\n",
      " 1   id                  1157 non-null   int64 \n",
      " 2   text                1156 non-null   object\n",
      " 3   ownerUsername       1157 non-null   object\n",
      " 4   ownerProfilePicUrl  1157 non-null   object\n",
      " 5   timestamp           1157 non-null   object\n",
      " 6   likesCount          1157 non-null   int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 63.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postUrl</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ownerUsername</th>\n",
       "      <th>ownerProfilePicUrl</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>likesCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>17981320055390052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n4i1er</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T01:46:41.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18225095332247021</td>\n",
       "      <td>üòçüòçüî•üî•üî•</td>\n",
       "      <td>farid.zand1997</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T09:30:36.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18016249762974314</td>\n",
       "      <td>patm</td>\n",
       "      <td>andreprivet_</td>\n",
       "      <td>https://scontent-dub4-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T12:32:20.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>17980123316614267</td>\n",
       "      <td>@hoccein_hemati68 ÿß€åŸÜ ⁄©Ÿàÿµ⁄©ÿ¥ ÿ™Ÿà ÿß€åÿ±ÿßŸÜ ÿ®ŸàÿØ ŸÜŸáÿß€åÿ™...</td>\n",
       "      <td>kurd___boy666</td>\n",
       "      <td>https://scontent-lga3-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T13:14:43.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/p/Cz67N84Pezn/</td>\n",
       "      <td>18036756691566765</td>\n",
       "      <td>@amir_niarashid üí©ü§£üñï</td>\n",
       "      <td>kurd___boy666</td>\n",
       "      <td>https://scontent-lga3-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>2023-12-09T13:15:08.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    postUrl                 id  \\\n",
       "0  https://www.instagram.com/p/Cz67N84Pezn/  17981320055390052   \n",
       "1  https://www.instagram.com/p/Cz67N84Pezn/  18225095332247021   \n",
       "2  https://www.instagram.com/p/Cz67N84Pezn/  18016249762974314   \n",
       "3  https://www.instagram.com/p/Cz67N84Pezn/  17980123316614267   \n",
       "4  https://www.instagram.com/p/Cz67N84Pezn/  18036756691566765   \n",
       "\n",
       "                                                text   ownerUsername  \\\n",
       "0                                                NaN          n4i1er   \n",
       "1                                              üòçüòçüî•üî•üî•  farid.zand1997   \n",
       "2                                               patm    andreprivet_   \n",
       "3  @hoccein_hemati68 ÿß€åŸÜ ⁄©Ÿàÿµ⁄©ÿ¥ ÿ™Ÿà ÿß€åÿ±ÿßŸÜ ÿ®ŸàÿØ ŸÜŸáÿß€åÿ™...   kurd___boy666   \n",
       "4                                @amir_niarashid üí©ü§£üñï   kurd___boy666   \n",
       "\n",
       "                                  ownerProfilePicUrl  \\\n",
       "0  https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "1  https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "2  https://scontent-dub4-1.cdninstagram.com/v/t51...   \n",
       "3  https://scontent-lga3-2.cdninstagram.com/v/t51...   \n",
       "4  https://scontent-lga3-2.cdninstagram.com/v/t51...   \n",
       "\n",
       "                  timestamp  likesCount  \n",
       "0  2023-12-09T01:46:41.000Z           0  \n",
       "1  2023-12-09T09:30:36.000Z           0  \n",
       "2  2023-12-09T12:32:20.000Z           0  \n",
       "3  2023-12-09T13:14:43.000Z           0  \n",
       "4  2023-12-09T13:15:08.000Z           0  "
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove comments with empty text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove text that is not related to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\n",
    "    data[\"text\"].apply(lambda x: all([token.pos_ != \"FOREIGN\" for token in nlp(x)]))\n",
    "    == True\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert timestamp to a standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non-ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: \"\".join([ch for ch in x if ord(ch) < 128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].str.replace(\"[^\\w\\s]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "data[\"text\"] = data[\"text\"].apply(\n",
    "    lambda x: \" \".join([word for word in x.split() if word not in stop_words])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"text\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows where the length of text is not greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: len(str(x)) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_the_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for sublist in preprocessed_data for word in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigram language model is a simple yet powerful statistical model used in natural language processing (NLP) to predict the probability of occurrence of a word in a sequence of words. It assumes that the probability of a word appearing in a sentence is independent of the context in which it appears. This assumption, while not entirely accurate, often provides a reasonable approximation of the true word distribution in a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class UnigramModel:\n",
    "    def __init__(self, smoothing=True):\n",
    "        self.word_counts = Counter()\n",
    "        self.total_words = 0\n",
    "        self.vocab_size = 0\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Count the frequency of each word in the corpus\n",
    "        self.word_counts = Counter(corpus)\n",
    "        self.total_words = len(corpus)\n",
    "        self.vocab_size = len(set(corpus))\n",
    "\n",
    "    def generate_sentence(self, length=10):\n",
    "        # Generate a random sentence based on the unigram model\n",
    "        sentence = [random.choice(list(self.word_counts.keys())) for _ in range(length)]\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def probability(self, word):\n",
    "        # Calculate the probability of a word based on the unigram model with smoothing\n",
    "        if self.total_words == 0:\n",
    "            return 0\n",
    "\n",
    "        if self.smoothing:\n",
    "            return (self.word_counts[word] + 1) / (self.total_words + self.vocab_size)\n",
    "        else:\n",
    "            return self.word_counts[word] / self.total_words\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        # Calculate perplexity of a sentence\n",
    "        words = sentence.split()\n",
    "        word_probabilities = [self.probability(word) for word in words]\n",
    "        perplexity = math.exp(-sum(math.log(prob, 2) for prob in word_probabilities) / len(words))\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnigramModel(smoothing=True)\n",
    "model.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate 5 sentences and calculate perplexity\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: bring full te gene deixem pasa gratulations zz na vahid.\n",
      "Perplexity: 55689.98\n",
      "Generated Sentence: marinhosjulio sin leostronda glen title international convidas mesi happiest thanks.\n",
      "Perplexity: 106200.94\n",
      "Generated Sentence: fucking parece veteran fogo thanks multiverse monstra levanta rico vida.\n",
      "Perplexity: 74899.53\n",
      "Generated Sentence: permitir dirty fdss frontal imo think qualquer engordaria dica mm.\n",
      "Perplexity: 100166.79\n",
      "Generated Sentence: respect children sound vou fome bonito hip 2am easy zinda.\n",
      "Perplexity: 87763.34\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generated_sentence = model.generate_sentence()\n",
    "    perplexity = model.perplexity(generated_sentence)\n",
    "    print(f\"Generated Sentence: {generated_sentence}.\\nPerplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigram language model is a statistical language model that predicts the probability of a word appearing in a sequence of words based on the word that precedes it. Unlike unigram models, which assume that words occur independently of each other, bigram models take into account the sequential nature of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self, smoothing=True):\n",
    "        self.bigram_counts = Counter()\n",
    "        self.unigram_counts = Counter()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Tokenize and count unigrams and bigrams in the corpus\n",
    "        tokenized_corpus = [word_tokenize(' '.join(sentence)) for sentence in corpus]\n",
    "        self.unigram_counts = Counter([word for sentence in tokenized_corpus for word in sentence])\n",
    "        self.bigram_counts = Counter(bigrams([word for sentence in tokenized_corpus for word in sentence]))\n",
    "\n",
    "    def generate_sentence(self, length=10, start_word=None):\n",
    "        # Generate a random sentence based on the bigram model\n",
    "        sentence = []\n",
    "        current_word = start_word if start_word else random.choice(list(self.unigram_counts.keys()))\n",
    "\n",
    "        for _ in range(length):\n",
    "            sentence.append(current_word)\n",
    "            next_word_candidates = [word for word in self.unigram_counts.keys() if (current_word, word) in self.bigram_counts]\n",
    "            if next_word_candidates:\n",
    "                current_word = random.choice(next_word_candidates)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def probability(self, word, previous_word=None):\n",
    "        # Calculate the probability of a word based on the bigram model with smoothing\n",
    "        if self.unigram_counts[previous_word] == 0:\n",
    "            return 0\n",
    "\n",
    "        if self.smoothing:\n",
    "            bigram_count = self.bigram_counts[(previous_word, word)]\n",
    "            unigram_count = self.unigram_counts[previous_word]\n",
    "            return (bigram_count + 1) / (unigram_count + len(self.unigram_counts))\n",
    "        else:\n",
    "            if (previous_word, word) in self.bigram_counts:\n",
    "                return self.bigram_counts[(previous_word, word)] / self.unigram_counts[previous_word]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        # Calculate perplexity of a sentence\n",
    "        words = word_tokenize(sentence)\n",
    "        word_probabilities = [self.probability(words[i], words[i - 1]) for i in range(1, len(words))]\n",
    "        word_probabilities = [prob if prob > 0 else 1e-10 for prob in word_probabilities]  # Avoid zero probabilities\n",
    "        perplexity = math.exp(-sum(math.log(prob, 2) for prob in word_probabilities) / (len(words) - 1)) if sum(word_probabilities) > 0 else float('inf')\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Generated Sentence: I\n",
      "Bigram Perplexity: inf\n",
      "---\n",
      "Bigram Generated Sentence: I\n",
      "Bigram Perplexity: inf\n",
      "---\n",
      "Bigram Generated Sentence: I\n",
      "Bigram Perplexity: inf\n",
      "---\n",
      "Bigram Generated Sentence: I\n",
      "Bigram Perplexity: inf\n",
      "---\n",
      "Bigram Generated Sentence: I\n",
      "Bigram Perplexity: inf\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Bigram model\n",
    "bigram_model = BigramModel(smoothing=True)\n",
    "bigram_model.train(data[\"text\"])\n",
    "\n",
    "# Generate and print sentences along with perplexity for both models\n",
    "for _ in range(5):\n",
    "    generated_bigram_sentence = bigram_model.generate_sentence(length=8, start_word=\"I\")\n",
    "    perplexity_bigram = bigram_model.perplexity(generated_bigram_sentence)\n",
    "    print(\"Bigram Generated Sentence:\", generated_bigram_sentence)\n",
    "    print(\"Bigram Perplexity:\", perplexity_bigram)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trigram language model is a statistical language model that predicts the probability of a word appearing in a sequence of words based on the two preceding words. It takes into account the sequential nature of language by considering the dependencies between three consecutive words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigramModel(token1, token2, token3, dataset, alpha=1):\n",
    "    token_count = Counter(dataset)\n",
    "    trigram_count = Counter(zip(dataset, dataset[1:], dataset[2:]))\n",
    "    bigram_count = Counter(zip(dataset, dataset[1:]))\n",
    "    return (trigram_count[(token1, token2, token3)] + alpha) / (bigram_count[(token1, token2)] + alpha * vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate perplexity base on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, n, dataset, alpha=1):\n",
    "    probabilities = [model(*dataset[i-n+1:i+1], dataset[:i], alpha) for i in range(n-1, len(dataset))]\n",
    "    perplexity = np.exp(-np.mean(np.log(probabilities)))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate sentence base on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, n, dataset, length=10):\n",
    "    sentence = []\n",
    "    for _ in range(length):\n",
    "        if n == 1:\n",
    "            next_token = random.choice(dataset)\n",
    "        else:\n",
    "            prev_tokens = sentence[-n+1:] if n > 1 else []\n",
    "            candidates = [token for token in set(dataset) if model(*(prev_tokens + [token]), dataset) > 0]\n",
    "            next_token = random.choice(candidates)\n",
    "        sentence.append(next_token)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate sentence perplexity base on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_perplexity(model, n, sentence, dataset, alpha=1):\n",
    "    tokens = tokenize_the_text(sentence)\n",
    "    probabilities = [model(*tokens[i-n+1:i+1], dataset, alpha) for i in range(n-1, len(tokens))]\n",
    "    perplexity = np.exp(-np.mean(np.log(probabilities)))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_report(model, n, dataset, alpha=1, num_sentences=5):\n",
    "    print(f\"Generating and evaluating {num_sentences} sentences for {n}-gram model:\")\n",
    "    for i in range(num_sentences):\n",
    "        generated_sentence = generate_sentence(model, n, dataset)\n",
    "        perplexity = calculate_sentence_perplexity(model, n, generated_sentence, dataset, alpha)\n",
    "        print(f\"Generated Sentence {i+1}: {generated_sentence}\")\n",
    "        print(f\"Perplexity: {perplexity}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram model sentence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and evaluating 5 sentences for 1-gram model:\n",
      "Generated Sentence 1: quase cbum homem terrified digo faa fazer cbum eat cbum\n",
      "Perplexity: 823.8398049227533\n",
      "\n",
      "Generated Sentence 2: vou um aqui ya stage cbum pradocamanda ta meto kenniemalm\n",
      "Perplexity: 1160.4304994252616\n",
      "\n",
      "Generated Sentence 3: fair long shape engaged um competir esqueciiiiiii elvis lives bumesteeeeeeeed\n",
      "Perplexity: 1557.8236700403875\n",
      "\n",
      "Generated Sentence 4: time se 5 nos expressar el enough perfeita welt todos\n",
      "Perplexity: 1194.433395457165\n",
      "\n",
      "Generated Sentence 5: tanta youre quiser went please esse pois timo mas fans\n",
      "Perplexity: 1299.4920094390195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_and_report(unigramModel, 1, total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram model sentence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and evaluating 5 sentences for 2-gram model:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bigramModel() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[729], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_and_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbigramModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[727], line 4\u001b[0m, in \u001b[0;36mgenerate_and_report\u001b[1;34m(model, n, dataset, alpha, num_sentences)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating and evaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_sentences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sentences for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-gram model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sentences):\n\u001b[1;32m----> 4\u001b[0m     generated_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m calculate_sentence_perplexity(model, n, generated_sentence, dataset, alpha)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Sentence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[725], line 8\u001b[0m, in \u001b[0;36mgenerate_sentence\u001b[1;34m(model, n, dataset, length)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     prev_tokens \u001b[38;5;241m=\u001b[39m sentence[\u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m----> 8\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(dataset) \u001b[38;5;28;01mif\u001b[39;00m model(\u001b[38;5;241m*\u001b[39m(prev_tokens \u001b[38;5;241m+\u001b[39m [token]), dataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(candidates)\n\u001b[0;32m     10\u001b[0m sentence\u001b[38;5;241m.\u001b[39mappend(next_token)\n",
      "Cell \u001b[1;32mIn[725], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     prev_tokens \u001b[38;5;241m=\u001b[39m sentence[\u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m----> 8\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(dataset) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(candidates)\n\u001b[0;32m     10\u001b[0m sentence\u001b[38;5;241m.\u001b[39mappend(next_token)\n",
      "\u001b[1;31mTypeError\u001b[0m: bigramModel() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "generate_and_report(bigramModel, 2, total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trigram model sentence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_and_report(trigramModel, 3, total_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
